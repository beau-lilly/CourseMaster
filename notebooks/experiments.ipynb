{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a4fda1da",
      "metadata": {},
      "source": [
        "### Prompt Engineering Experiment\n",
        "\n",
        "Problems are straight from the CS 372 Practice Midterms. I will reference the content of each problem below the table. For each problem, the structure of the prompt that is being passed to the LLM is the following: \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "You are a helpful assistant. The user is working on the problem below.\n",
        "\n",
        "Problem:\n",
        "{problem}\n",
        "\n",
        "Answer the user's question about this problem based on the provided context.\n",
        "Keep your answer brief and to the point.\n",
        "\n",
        "Context (retrieved chunks related to the problem):\n",
        "{context}\n",
        "\n",
        "Question about the problem: {question}\n",
        "Answer:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "**Style Instructions:**\n",
        "\n",
        "*Minimal*: You are a helpful assistant. Answer the user's question based ONLY on the provided context. Keep your answer brief and to the point.\n",
        "\n",
        "*Explanatory*: You are an expert tutor. Answer the user's question using the provided context. You must cite the specific chunk index (e.g., [Chunk 1]) that supports each part\n",
        "\n",
        "*Tutoring*: You are a Socratic tutor. Do not give the answer directly. Instead, use the context to guide the user toward the answer with a hint or a leading question.\n",
        "\n",
        "*Similarity*: Analyze why the following chunks were retrieved for the user's question. Explain the relevance of each chunk to the query.\n",
        "\n",
        "\n",
        "For this experiment, the \"question\" part is kept the same: \"Please explain how I should approach this problem\" and only the problem and style were changed‚Äîapplying each style to each problem. The quantitative outputs were recorded below, and the outputs of the notebook were \n",
        "\n",
        "\n",
        "| Problem | Prompt Style | Answer latency (s) | Answer length (words) |\n",
        "| --- | --- | --- | --- |\n",
        "| Practice Midterm 1 #8 | Minimal | 5.8 | 193 |\n",
        "| Practice Midterm 1 #8 | Explanatory | 11.4 | 435 |\n",
        "| Practice Midterm 1 #8 | Tutoring | 4.6 | 174 |\n",
        "| Practice Midterm 1 #8 | Similar | 10.6 | 475 |\n",
        "| Practice Midterm 1 #12 | Minimal | 3.9 | 105 |\n",
        "| Practice Midterm 1 #12 | Explanatory | 8.5 | 333 |\n",
        "| Practice Midterm 1 #12 | Tutoring | 5.0 | 180 |\n",
        "| Practice Midterm 1 #12 | Similar | 10.4 | 490 |\n",
        "| Practice Midterm 1 #14 | Minimal | 2.8 | 73 |\n",
        "| Practice Midterm 1 #14 | Explanatory | 9.4 | 339 |\n",
        "| Practice Midterm 1 #14 | Tutoring | 3.2 | 120 |\n",
        "| Practice Midterm 1 #14 | Similar | 15.7 | 504 |\n",
        "| Practice Midterm 1 #15 | Minimal | 3.9 | 99 |\n",
        "| Practice Midterm 1 #15 | Explanatory | 9.7 | 302 |\n",
        "| Practice Midterm 1 #15 | Tutoring | 2.4 | 79 |\n",
        "| Practice Midterm 1 #15 | Similar | 12.4 | 481 |\n",
        "| Practice Midterm 2 #8 | Minimal | 2.8 | 67 |\n",
        "| Practice Midterm 2 #8 | Explanatory | 11.9 | 307 |\n",
        "| Practice Midterm 2 #8 | Tutoring | 1.0 | 49 |\n",
        "| Practice Midterm 2 #8 | Similar | 12.1 | 536 |\n",
        "| Practice Midterm 2 #9 | Minimal | 2.7 | 57 |\n",
        "| Practice Midterm 2 #9 | Explanatory | 9.1 | 318 |\n",
        "| Practice Midterm 2 #9 | Tutoring | 3.1 | 111 |\n",
        "| Practice Midterm 2 #9 | Similar | 16.4 | 466 |\n",
        "| Practice Midterm 2 #10 | Minimal | 2.1 | 82 |\n",
        "| Practice Midterm 2 #10 | Explanatory | 7.3 | 273 |\n",
        "| Practice Midterm 2 #10 | Tutoring | 1.3 | 51 |\n",
        "| Practice Midterm 2 #10 | Similar | 11.8 | 523 |\n",
        "| Practice Midterm 2 #18 | Minimal | 2.6 | 80 |\n",
        "| Practice Midterm 2 #18 | Explanatory | 11.7 | 46 |\n",
        "| Practice Midterm 2 #18 | Tutoring | 1.7 | 46 |\n",
        "| Practice Midterm 2 #18 | Similar | 12.9 | 499 |\n",
        "\n",
        "\n",
        "### Problem Content\n",
        "\n",
        "**Practice Midterm 1 #8**: \"Logistic regression models are typically trained to minimize the Cross-Entropy Loss (CE). Does a model with strictly positive CE on a given training dataset necessarily have less than 100% classificaiton accuracy on that same dataset? Briefly explain.\"\n",
        "\n",
        "**Practice Midterm 1 #12**: \"Name and define (mathematically) the function most commonly used for normalizing the outputs of a neural network for a classification task into k>2 categories.\"\n",
        "\n",
        "**Practice Midterm 1 #14**: \"Both back propagation and the stochastic gradient descent algorithm are commonly used in training artificial neural networks. Briefly describe the difference between these two algorithms.\"\n",
        "\n",
        "**Practice Midterm 1 #15**: \"You are training a neural network with minibatch stochastic gradient descent. Briefly describe what the learning rate hyperparameter is. Why might you increase the learning rate? Why might you decrease the learning rate?\"\n",
        "\n",
        "**Practice Midterm 2 #8**: \"Describe the Reinforcement Learning from Human Feedback (RLHF) paradigm for language model alignment. How does it differ from supervised fine-tuning (SFT) or instruction-tuning?\"\n",
        "\n",
        "**Practice Midterm 2 #9**: \"Describe the technique of gradient accumulation during model training or fine-tuning. Explain how gradient accumulation allows you to maintain a greater effective batch size during training or fine-tuning while reducing memory requirements.\"\n",
        "\n",
        "**Practice Midterm 2 #10**: \"For a causal decoder language model, where should the system prompt be inserted alongside the user prompt in the input to the model? Briefly justify your answer with reference to the attention mechanism.\"\n",
        "\n",
        "**Practice Midterm 2 #18**: \"In Reinforcement Learning, we want to learn a policy. A policy is a function from what domain to what\n",
        "range? (In other words, what is the set of possible inputs to the policy, and what is the set of possible\n",
        "outputs?)\"\n",
        "\n",
        "\n",
        "### Qualitative Analysis\n",
        "The style instructions labeled at the top of this cell describe the outputs very precisely, so I will provide a more pragmatic analysis‚Äîhow the various prompts can be used most effectively. \n",
        "- Miminal: used to check understanding quickly and get instantaneous feedback if one already has an answer and is confident in their understanding. \n",
        "- Explanatory: If a student is completely lost on a subject, or it is one of the first problems they have seen of this type, the explanatory response gives a good formula for how to solve these kinds of problems. \n",
        "- Tutoring: Once a student has seen a few of the same types of problems (covering the same content or using the same techniques), they can use the tutoring prompt if they are almost to the answer but are missing a small amount of vital information. The tutoring prompt will lead them back through the process of discovering the answer by explaining where in the notes the relevant content is, presumably leading the student to find the vital piece of missing information on their own. \n",
        "- Similarity: This proved to be the least helpful, but I think if I used semantic chunking rather than a fixed character limit, and used a threshold matching algorithm rather than top k (this combination would ensure highly relevant information to the Problem), this would result in this prompt being much more useful to the student. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "207e23a0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Problem: In Reinforcement Learning, we want to learn a policy. A policy is a function from what domain to what\n",
            "range? (In other ...\n",
            "Question: Please explain how I should approach this problem\n",
            "\n",
            "---\n",
            "Prompt style: minimal\n",
            "Latency (ms): 2591.23\n",
            "Answer length (words): 80\n",
            "Answer: To approach this problem, you should focus on learning the optimal policy (ùúã‚àó) and the optimal value function (ùë£‚àó) by interacting with the environment. Use the Bellman Optimality Equation to iteratively compute the optimal value function, which will help you infer the optimal policy. Consider the dynamics of the environment and the rewards associated with actions in different states. If the environment is deterministic, you can simplify your calculations. Aim to maximize the expected return (ùîºùê∫ùë°) through your chosen actions.\n",
            "\n",
            "---\n",
            "Prompt style: explanatory\n",
            "Latency (ms): 9145.06\n",
            "Answer length (words): 319\n",
            "Answer: To approach the problem effectively, you should follow these steps:\n",
            "\n",
            "1. **Understand the Environment**: Begin by familiarizing yourself with the Markov Decision Process (MDP) framework, which consists of states (ùíÆ), actions (ùíú), and rewards (‚Ñõ). Recognize that valid actions depend on the current state, and the environment dynamics are defined by the transition probabilities, which indicate the likelihood of moving from one state to another given a specific action [Chunk 1].\n",
            "\n",
            "2. **Define Your Policy**: A policy (ùúã) is a function that assigns probabilities to actions based on the current state. Ensure you understand how to represent this policy mathematically, as it will guide your decision-making process [Chunk 4].\n",
            "\n",
            "3. **Calculate Value Functions**: Learn how to compute the state value function (ùë£ùúã ùë†), which represents the expected return starting from state ùë† and following policy ùúã. This is crucial for evaluating how good a particular state is under a given policy [Chunk 4].\n",
            "\n",
            "4. **Aim for Optimality**: Your goal is to find the optimal policy (ùúã‚àó) and the optimal value function (ùë£‚àó ùë†). The optimal value function gives the maximum expected return over all possible policies. Familiarize yourself with the Bellman Optimality Equation, which provides a recursive way to compute the optimal value function [Chunk 5].\n",
            "\n",
            "5. **Iterate Using Value Iteration**: If you know the environment dynamics, you can apply the Bellman Optimality Equation iteratively to compute the optimal value function to arbitrary precision. This method is known as value iteration and is a common approach in model-based reinforcement learning [Chunk 6].\n",
            "\n",
            "6. **Consider Discounting**: In scenarios where future rewards are less valuable than immediate rewards, apply a discount factor (ùõæ) to your calculations. This factor will help you balance the importance of immediate versus future rewards, which is essential for maximizing the expected return [Chunk 4].\n",
            "\n",
            "By following these steps, you will systematically build your understanding and approach to solving the problem within the context of reinforcement learning.\n",
            "\n",
            "---\n",
            "Prompt style: tutoring\n",
            "Latency (ms): 1173.4\n",
            "Answer length (words): 46\n",
            "Answer: To approach this problem, consider the key concepts of value functions and policies in reinforcement learning. What do you think is the relationship between the value function and the actions taken by a policy? How might the Bellman Optimality Equation help you understand this relationship better?\n",
            "\n",
            "---\n",
            "Prompt style: similarity\n",
            "Latency (ms): 12957.71\n",
            "Answer length (words): 499\n",
            "Answer: To analyze the relevance of each chunk retrieved for the user's question, we need to consider the context of the question, which is asking for guidance on how to approach a problem related to reinforcement learning (RL) and Markov Decision Processes (MDPs). The retrieved chunks provide foundational concepts and equations that are essential for understanding and solving problems in this domain. Here‚Äôs a breakdown of the relevance of each chunk:\n",
            "\n",
            "### Chunk 5\n",
            "- **Content Summary**: This chunk discusses the value function in reinforcement learning, optimal policies, and the Bellman Optimality Equation.\n",
            "- **Relevance**: Understanding the value function and optimal policies is crucial for approaching RL problems. The Bellman Optimality Equation is a key tool for deriving optimal policies and value functions, which are central to solving MDPs. This chunk provides the mathematical framework necessary for the user to formulate and solve their problem effectively.\n",
            "\n",
            "### Chunk 4\n",
            "- **Content Summary**: This chunk explains the concept of discounted returns and the intuition behind discounting in reinforcement learning.\n",
            "- **Relevance**: The concept of discounted returns is fundamental in RL, as it affects how future rewards are valued compared to immediate rewards. This understanding is essential for formulating strategies in RL problems, especially when deciding how to balance short-term and long-term rewards. The user needs to grasp this concept to make informed decisions in their approach.\n",
            "\n",
            "### Chunk 1\n",
            "- **Content Summary**: This chunk introduces the setting of reinforcement learning, including the learner's observations, actions, and the Markov Decision Process (MDP).\n",
            "- **Relevance**: This foundational overview sets the stage for understanding how RL operates within the framework of MDPs. It outlines the basic components (states, actions, rewards) that the user must consider when approaching their problem. This context is vital for any further analysis or application of RL techniques.\n",
            "\n",
            "### Chunk 6\n",
            "- **Content Summary**: This chunk reiterates the Bellman Optimality Equation and discusses how to compute the optimal value function and policy through value iteration.\n",
            "- **Relevance**: The mention of value iteration as a method for computing the optimal value function is directly applicable to many RL problems. It provides a practical algorithmic approach that the user can implement to solve their problem. Understanding this process is essential for anyone looking to apply RL techniques effectively.\n",
            "\n",
            "### Chunk 3\n",
            "- **Content Summary**: This chunk discusses episodic tasks, terminal states, and the concept of returns in the context of MDPs.\n",
            "- **Relevance**: The distinction between episodic and continuing tasks is important for the user to understand the nature of the problem they are tackling. Knowing how to calculate returns in different scenarios will help the user structure their approach and choose the right methods for their specific problem.\n",
            "\n",
            "### Conclusion\n",
            "Overall, each chunk contributes to a comprehensive understanding of reinforcement learning and MDPs, providing the user with the necessary theoretical background and practical tools to approach their problem. The combination of foundational concepts, mathematical formulations, and algorithmic strategies equips the user to analyze their specific situation and devise an effective solution.\n"
          ]
        }
      ],
      "source": [
        "# code used to gather data\n",
        "\n",
        "import time\n",
        "from src.core.database import DatabaseManager\n",
        "from src.core.rag import answer_question\n",
        "from src.core.types import PromptStyle\n",
        "\n",
        "db = DatabaseManager()\n",
        "\n",
        "course = db.get_course_by_name(\"CS 372\")\n",
        "if not course:\n",
        "    raise ValueError(\"Course 'CS 372' not found.\")\n",
        "\n",
        "exam = db.get_exam_by_name(course.course_id, \"Final\")\n",
        "if not exam:\n",
        "    raise ValueError(\"Exam 'Final' not found for course 'CS 372'.\")\n",
        "\n",
        "problems = db.list_problems_for_exam(exam.exam_id)\n",
        "if not problems:\n",
        "    raise ValueError(\"No problems found for the Final exam in CS 372.\")\n",
        "\n",
        "target_problem = problems[0]\n",
        "question_text = \"Please explain how I should approach this problem\"\n",
        "prompt_styles = [\n",
        "    PromptStyle.MINIMAL,\n",
        "    PromptStyle.EXPLANATORY,\n",
        "    PromptStyle.TUTORING,\n",
        "    PromptStyle.SIMILARITY,\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for style in prompt_styles:\n",
        "    start = time.perf_counter()\n",
        "    result = answer_question(\n",
        "        question_text=question_text,\n",
        "        problem_id=target_problem.problem_id,\n",
        "        prompt_style=style,\n",
        "        db_manager=db,\n",
        "    )\n",
        "    elapsed = time.perf_counter() - start\n",
        "    rows.append(\n",
        "        {\n",
        "            \"prompt_style\": style.value,\n",
        "            \"latency_ms\": round(elapsed * 1000, 2),\n",
        "            \"word_count\": len(result.answer.split()),\n",
        "            \"answer\": result.answer,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(f\"Problem: {target_problem.problem_text[:120]}...\")\n",
        "print(f\"Question: {question_text}\")\n",
        "for row in rows:\n",
        "    print(\"\\n---\")\n",
        "    print(f\"Prompt style: {row['prompt_style']}\")\n",
        "    print(f\"Latency (ms): {row['latency_ms']}\")\n",
        "    print(f\"Answer length (words): {row['word_count']}\")\n",
        "    print(f\"Answer: {row['answer']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "413f70e2",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
